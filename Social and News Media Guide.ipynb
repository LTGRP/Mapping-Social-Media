{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media and News Analysis Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this guide is to provide a guide to some of the tools that can be used to analyze social media and news networks. \n",
    "\n",
    "Software that need to be pre-installed: \n",
    "\n",
    "(1) Python (2.7 or later) \n",
    "https://www.python.org/downloads/windows/\n",
    "<br>\n",
    "(2) Gephi\n",
    "https://gephi.org/\n",
    "\n",
    "** Note: The Images folder that is associated with this file should remain within the same directory as the file to be able to view the images. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Social Landscape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of the Twitter social landscape can be divided into three steps:\n",
    "<br> \n",
    "(1) Data Collection\n",
    "<br>\n",
    "(2) Data Visualization\n",
    "<br>\n",
    "(3) Data Analysis\n",
    "<br>\n",
    "<br> \n",
    "<p style=\"font-size:160%;\"> **Data Collection** </p>\n",
    "<br>\n",
    "The normal Python Twitter API (tweepy) doesn't allow a user to extract data that is more than 7 days old. As a result, historical trends can't be visualized using solely the tweepy API. \n",
    "<br> \n",
    "Luckily, Jefferson-Henrique developed a Python Script under the MIT License (unrestricted software re-use) that can be used to download JSON files containing tweet data. The script can be found here: https://github.com/Jefferson-Henrique/GetOldTweets-python. \n",
    "<br>\n",
    "<br>\n",
    "** How to Use This Script ** \n",
    "<br>\n",
    "The details of the Python JSON download can be found in the Manager.py file in the \"got\" folder. However, the Main function is where tweet search criteria can be altered. Below are details on basic query searches that can be altered to fit search needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<got.models.Tweet.Tweet instance at 0x04F21BC0>\n",
      "<got.models.Tweet.Tweet instance at 0x050E9C10>\n",
      "<got.models.Tweet.Tweet instance at 0x04F222D8>\n",
      "<got.models.Tweet.Tweet instance at 0x04F220D0>\n",
      "<got.models.Tweet.Tweet instance at 0x050E9DC8>\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import unicodedata\n",
    "import csv\n",
    "import got\n",
    "from IPython.core.display import HTML\n",
    "    \n",
    "# Setting up \n",
    "KEYWORD = \"forest restoration\" # Sets the keyword to look for in the tweet\n",
    "START_DATE = \"2017-07-01\" # Sets the start date for the search (must be in YYYY-MM-DD format as a string)\n",
    "END_DATE = \"2017-07-10\" # Sets the end date for the search\n",
    "tweetCriteria = got.manager.TweetCriteria().setQuerySearch(KEYWORD).setSince(START_DATE).setUntil(END_DATE).setMaxTweets(5)\n",
    "results = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "\n",
    "for i in range(len(results)):\n",
    "    print results[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "OK, so each of the results appears to be an object. Each tweet object has several attributes (see https://github.com/Jefferson-Henrique/GetOldTweets-python/blob/master/README.md for more details). The two main attributes that allow for network visualization are \"username\" and \"mentions\" which provide information on \n",
    "<br>\n",
    "(a) The screen name of the sender of the tweet\n",
    "<br>\n",
    "(b) The screen name of the recipient of the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creates an empty source and target list\n",
    "source = []\n",
    "target = []\n",
    "# Loops over each result to parse the data stored in the result\n",
    "for i in range(len(results)):\n",
    "    # Stores each result in a temporary variable called \"tweet\"\n",
    "    tweet = results[i]\n",
    "    # Stores the username of the creator of content in \"username\" as a string \n",
    "    username = tweet.username\n",
    "    # Stores the list of mentions in the tweet in \"mentions\" as a string\n",
    "    mentions = tweet.mentions\n",
    "    # Accounts for the case where no other user is mentioned in the tweet\n",
    "    # Stores the username of the creator of content in both the source and target lists\n",
    "    if \"@\" not in mentions:\n",
    "        source.append(username)\n",
    "        target.append(username)\n",
    "    # Counts the number of other accounts mentioned in the tweet using the \"@\" character\n",
    "    # For each mention, the username and the specific account mentioned are appended to the source and target lists respectively\n",
    "    else:\n",
    "        number_of_mentions = mentions.count(\"@\") + 1\n",
    "        split_mentions = mentions.split(\"@\")\n",
    "        for i in range(number_of_mentions):\n",
    "            if i > 0:\n",
    "                source.append(username)\n",
    "                target.append(split_mentions[i])\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, we have two lists (source and target) that contain the identities of authors and mentions in tweets. \n",
    "<br>\n",
    "Now, all that is left is to print the data to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specifies the filename to store the source and target columns (REMEMBER TO ADD .csv AT THE END OF THE FILENAME)\n",
    "filename = \"ForestRestorationTweets.csv\"\n",
    "with open(filename, 'wb') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    # Writes the source and target lists for each row in the CSV file\n",
    "    for i in range(len(source)):\n",
    "        text = source[i],target[i]\n",
    "        writer.writerow(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a CSV file with two columns: source and target. This file can be uploaded to Gephi to visualize the network surrounding forest restoration. \n",
    "<br> \n",
    "<br> \n",
    "But before getting into social media network visualization, there are several other types of searches that can be performed by the GOT manager. \n",
    "<br>\n",
    "For instance, if you wanted to track all of the tweets posted by \"restoreforward\" and find who is being mentioned in the tweets, you could instead use the TweetCriteria, setUsername()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? n\n",
      "Nothing done.\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@SofiaFaruqi @ABCaustralia\n",
      "@AlertNet\n",
      "@GreenBiz\n",
      "@ABCaustralia @SofiaFaruqi\n",
      "@paapPeter @globalforests\n",
      "@FAOForestry\n",
      "@WorldResources\n",
      "@rachel_biderman @JeffBezos @SofiaFaruqi @WorldResources\n",
      "@WRIIndia\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import unicodedata\n",
    "import csv\n",
    "import got\n",
    "from IPython.core.display import HTML\n",
    "    \n",
    "# Setting up \n",
    "USERNAME = \"restoreforward\"\n",
    "START_DATE = \"2017-06-01\"\n",
    "END_DATE = \"2017-07-10\"\n",
    "# Sets the parameters of the search through the TweetCriteria class in got.manager\n",
    "# List of TweetCriteria parameters:\n",
    "# setUsername, setSince, setUntil, setQuerySearch, setMaxTweets, setTopTweets, setNear, setWithin\n",
    "tweetCriteria = got.manager.TweetCriteria().setUsername(USERNAME).setSince(START_DATE).setUntil(END_DATE)\n",
    "results = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "# Loops through each result to see each account mentioned in each tweet by \"restoreforward\"\n",
    "for i in range(len(results)):\n",
    "    if results[i].mentions != \"\":\n",
    "        print results[i].mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the different twitter accounts that \"restoreforward\" has tweeted to since the start of 2017. This data can also be outputted to a CSV file to visualize network maps for different Twitter accounts.\n",
    "<br> \n",
    "<br> \n",
    "Finally, it is possible to text mine tweets for their content. Doing this makes it possible to look at the most frequently mentioned words, word groupings, and overall sentiment, tone, and polarity of a range of tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an empty list to store tweet text\n",
    "tweet_text = []\n",
    "# Loops through each result to append the tweet text to the tweet text list\n",
    "for i in range(len(results)):\n",
    "    tweet_text.append(results[i].text)\n",
    "\n",
    "def printTweetText(text_file):\n",
    "    # Writes each element in the tweet text list to a text file \n",
    "    # REMEMBER TO ADD .txt TO THE END OF THE TEXT FILE NAME (eg. \"RestorationTweets.txt\")\n",
    "    with open(text_file, 'wb') as text_file:\n",
    "        for i in range(len(tweet_text)):\n",
    "            text = tweet_text[i].encode(sys.stdout.encoding, errors='replace')\n",
    "            text = ' ' + text\n",
    "            text_file.write(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p style=\"font-size:160%;\"> **Data Visualization** </p>\n",
    "<br>\n",
    "With a CSV file of source and target nodes, Gephi can be used to visualize the data using a network map, and provide basic tools for statistical analysis. Once Gephi is launched, it should look like this: \n",
    "<img src=\"Images\GephiHome.png\" style=\"width: 800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To upload a CSV file containing Twitter data, go to File -> Open -> \"Filename\". The graph can either be \"Mixed\" or \"Directed\" and the weight of parallel edges should be merged. \n",
    "<br>\n",
    "<img src=\"Images/ImportStatement.png\" style=\"width: 400px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting graph will look like a hodgepodge of many nodes and edges:\n",
    "<br>\n",
    "<img src=\"Images/1stgraph.png\" style=\"width: 300px;\">\n",
    "<br>\n",
    "However, this graph can be re-visualized using a different visualizations: \n",
    "<br> \n",
    "(a) Fruchterman-Reingold \n",
    "<br>\n",
    "(b) Yifan Hu\n",
    "<br> \n",
    "These two visualizations are the most popular, and provide insight into the structure of the network. They can be accessed by going to the layout tab, and selecting the appropriate visualization. Then, the visualization can be viewed either in the \"Graph\" or the \"Preview\" tab. \n",
    "<br> \n",
    "Below is a visualization of all tweets containing forest restoration in 2017. \n",
    "<img src=\"Images/2ndgraph.png\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:160%;\"> **Analysis** </p>\n",
    "<br>\n",
    "To analyze the network, Gephi offers several graph metrics that can be applied in the \"Overview\" pane (Overview -> Filters). \n",
    "<br> \n",
    "<img src=\"Images/FilterHome.png\" style=\"width: 200px;\">\n",
    "<br> \n",
    "Below is a description of the most popular filters, and what it can be used for: \n",
    "<br> \n",
    "<br> \n",
    "**Edge Weight**: This filters nodes and edges by the strength of their commmunication. For instance, if \"restoreforward\" and \"WorldResources\" had appeared 5 times in the dataset, they would have an edge weight of 5. \n",
    "<br> \n",
    "**Self Loop**: This filters nodes and edges by the presence of a self loop. This is useful to see if a given node is an originator of content, as a self loop would indicate a tweet that was written, but not tagged with other accounts. (Topology -> HasSelfLoop can also be used as a similar filter)\n",
    "<br> \n",
    "**Degree Range**: This generates a subgraph of minimum/maximum degree N (where N is selected by the user). Thus, each vertex in this newly generated subgraph must have had at least degree N in the original graph. \n",
    "<br> \n",
    "**Ego Network**: This generates a subgraph of a given node's connections within the network (a node ID, and a degree level need to be specified)\n",
    "<br>\n",
    "**K-Core**: This generates a subgraph where each node has at least degree N (where N is selected by the user). This is useful in studying network connectivity, and which sub-network of actors has the greatest strenghth between one another. \n",
    "<br> \n",
    "<br> \n",
    "Below are examples of graphs generated using some of these analysis metrics in the forest restoration network in 2017: \n",
    "<br> \n",
    "<img src=\"Images/EdgeGraph.png\" width=\"700\">\n",
    "<p style=\"text-align: center;\"> Edge-Weight > 3 </p>\n",
    "<img src=\"Images/DegreeGraph.png\" width=\"700\">\n",
    "<p style=\"text-align: center;\"> Degree Range > 5 </p>\n",
    "<img src=\"Images/EgoNetwork.png\" width=\"700\">\n",
    "<p style=\"text-align: center;\"> Ego Network (restoreforward - degree 3) </p>\n",
    "<img src=\"Images/KCore.png\" width=\"700\">\n",
    "<p style=\"text-align: center;\"> K-Core > 4 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gephi also allows users to perform various statistical analysis through the data table. To access these statistical tools, navigate to the statistics tab (Window -> Statistics). \n",
    "<img src=\"Images/Statistics.png\" width=\"250\">\n",
    "<br> \n",
    "Below is a description of the most commonly used statistical measures and their effects: \n",
    "<br> \n",
    "<br> \n",
    "**Average Degree**: Takes the average node degree in the network. \n",
    "$$ \\sum_{n=1}^{N}\\rho(v_{i}) = 2E \\hspace{1cm}  \\text{ and so the average degree of the graph is: } \\hspace{1cm}  deg_{avg} = \\frac{2E}{N} $$\n",
    "<br> \n",
    "**PageRank**: Takes the rank of each node's importance and relevance to the network. (similar to the PageRank algorithm used by search engines [Google, Bing, etc.]) The math behind the algorithm is quite complex, and won't be shown below. Please see https://en.wikipedia.org/wiki/PageRank and https://github.com/gephi/gephi/wiki/PageRank for more information. \n",
    "<br> \n",
    "<br>\n",
    "**Avg. Clustering Coefficient**: Returns a measure of the degree to which each node is grouped with other pairs of nodes. Provides coefficients for each node separately as well as the network as a whole. \n",
    "<br>\n",
    "Local Clustering:\n",
    "$$ C_{i} = \\frac{|e_{jk}: v_{j},v_{k}\\in N_{i}|}{|k_{N_{i}}|} $$\n",
    "<br>\n",
    "Network Clustering: \n",
    "$$ C_{network} = \\frac{1}{N}\\sum_{n=1}^{N}C_{i} $$\n",
    "<br>\n",
    "**Eigenvector Centrality**: Returns a scalar representing the influence of a node in a network. \n",
    "$$ A\\textbf{x}=\\lambda\\textbf{x} $$\n",
    "where A represents the adjacency matrix and x is a the vector sum over the set of a node's neighbors. The greatest eigenvalue: \n",
    "$$ \\lambda_{max} $$  is taken to be the centrality measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Analysis**\n",
    "<br> \n",
    "First, the text file should be loaded into a list which can then be parsed and analyzed using Python. Alternatively, the text file could be directly fed into the Provalis Text Mining tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text(filename): # Remember to including .txt at the end of filename\n",
    "    text_file = open(filename, 'r')\n",
    "    lines = text_file.read().split(' ') # Splits the text by a space\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at word frequencies, the get_frequencies method outlined below can be used to return a 2-dimensional list of each word in the document, along with its frequency. The filter_array method can be used to filter out words returrned through get_frequencies that are not relevant. (pronouns, conjunctions, etc.) This filtered list can be then outputted to a CSV file, where it is possible to do further analysis and/or visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_frequencies(lines):\n",
    "    # Counts the number of words in the text file\n",
    "    array_lines = range(len(lines))\n",
    "    # Creates an empty list to store word counts\n",
    "    word_array = []\n",
    "    # Loops over each word in the text file and determines whether it is already in the word array\n",
    "    # If it is not in the word array, it adds the word, with a word count of \"1\" to the word array\n",
    "    # If it is already in the word array, it increases the word count by 1\n",
    "    for i in array_lines:\n",
    "        if lines[i] not in [word[0] for word in word_array]:\n",
    "            word_array.append([lines[i],1])\n",
    "        else:\n",
    "            word_array_length = range(len(word_array))\n",
    "            for j in word_array_length:\n",
    "                if word_array[j][0] == lines[i]:\n",
    "                    word_array[j][1] += 1\n",
    "    # Sorts the word array by word count, and returns the word count in descending order (most frequent first)                \n",
    "    word_array = sorted(word_array, key=lambda x: int(x[1]))\n",
    "    word_array = list(reversed(word_array))\n",
    "    \n",
    "    return word_array\n",
    "\n",
    "def filter_array(word_array):\n",
    "    # Counts the number of distinct words in the word array\n",
    "    length_array = range(len(word_array))\n",
    "    # Creates a list of words that are \"fluff\" - words you don't want inlcuded in the analysis\n",
    "    fluff = [\"ENTER WHAT WORDS YOU WANT TO FILTER HERE\"]\n",
    "    # EXAMPLE:\n",
    "    # fluff = ['and','the','because','for','she','he','that','have','not']\n",
    "    # Loops over each word in the word array and kills the word counts for words that are either:\n",
    "    # (a) less than 3 letters long\n",
    "    # (b) in the fluff list\n",
    "    for i in length_array:\n",
    "        if len(word_array[i][0]) < 3:\n",
    "            word_array[i][1] = 0\n",
    "        if word_array[i][0] in fluff:\n",
    "            word_array[i][1] = 0\n",
    "    # Sorts the newly filtered word array by word count, and returns the word count in descending order (most frequent first)        \n",
    "    word_array = sorted(word_array, key=lambda x: int(x[1]))\n",
    "    word_array = list(reversed(word_array))\n",
    "    \n",
    "    return word_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing Remarks for Twitter Analysis\n",
    "<br> \n",
    "Although there are many more potential uses and applications for Twitter data analysis, this guide has laid out the foundations for the first three steps in using Twitter as a tool to engineer, understand, and monitor the progress of a movement. \n",
    "<br> <br>\n",
    "**(1)** Collect Twitter Data using the GOT manager\n",
    "<br> <br>\n",
    "**(a)** Select data based on: Username, Start Date, End Date, Query (keyword), Top Tweets, Location (near a given location), Radius (radius around a location), Maximum number of tweets\n",
    "<br>\n",
    "**(b)** Filter and extract data based on: Tweet ID, Tweet Permalink (URL), Username, Tweet Text, Tweet Date, Number of Retweets, Number of Favorites, Tweet Mentions, Tweet Hashtags, Tweet Geography\n",
    "<br> <br> \n",
    "**(2)** Load and visualize data using Gephi\n",
    "<br> <br> \n",
    "**(a)** Select from visualization methodologies such as: Fruchterman-Reingold, Yifan-Hu, Noverlap, OpenOrd, and ForceAtlas. \n",
    "<br>\n",
    "**(b)** Alter color, size, outline, and groupings of nodes and edges in the Gephi Preview. \n",
    "<br> <br> \n",
    "**(3)** Analyze data using Gephi statistical packages, Python scripts, and other software. \n",
    "<br> <br> \n",
    "**(a)** Get network metrics from Gephi such as: average degree, network diameter, modularity, PageRank, clustering coefficients, eigenvector centrality, and path length. \n",
    "<br> \n",
    "**(b)** Look at word frequencies through text mining software to study word patterns, tone, and polarity. \n",
    "<br>\n",
    "**(c)** Any other analyses you would like to conduct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
